TransformerConfig(_name='ptransformer', activation_fn='relu', dropout=0.3, attention_dropout=0.0, activation_dropout=0.0, adaptive_input=False, encoder=EncDecBaseConfig(_name='ptransformer', embed_path=None, embed_dim=512, ffn_embed_dim=2048, layers=6, attention_heads=8, normalize_before=False, learned_pos=False, layerdrop=0, layers_to_keep=None, xformers_att_config=None), max_source_positions=1024, decoder=DecoderConfig(_name='ptransformer', embed_path=None, embed_dim=512, ffn_embed_dim=2048, layers=6, attention_heads=8, normalize_before=False, learned_pos=False, layerdrop=0, layers_to_keep=None, xformers_att_config=None, input_dim=512, output_dim=512), max_target_positions=1024, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, layernorm_embedding=False, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, checkpoint_activations=False, offload_activations=False, no_cross_attention=False, cross_self_attention=False, quant_noise=QuantNoiseConfig(_name='ptransformer', pq=0, pq_block_size=8, scalar=0), min_params_to_wrap=100000000, char_inputs=False, relu_dropout=0.0, base_layers=0, base_sublayers=1, base_shuffle=1, export=False, no_decoder_final_norm=False)